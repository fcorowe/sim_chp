---
# output: bookdown::github_document2
output:
  bookdown::pdf_document2:
    template: template.tex
    keep_tex: true
bibliography: sim_refs.bib
editor_options:
  markdown:
    wrap: sentence
---

Overall aim: To set our view about how the field of spatial interaction modelling should evolve.

Key argument: Spatial interaction models are great, yet, not progress has been made over the last two/three decades (though we should recognise the work LeSage on spatial econometrics, Dan Griffith on spatial filtering, Kanaroglou et al on spatial discrete choice modelling, radiation models).
Key challenges have prevented progress:

1\.
reproducibility

2\.
calibration <!--# AD - to elaborate this idea of calibration. I have long struggled with the term calibration in the context of spatial interaction modelling. In the olden days,  people used calibration to describe curve fitting parameters in a mathemathical framework. In a statistical context, I would understand this as the steps taken for a model to converge. -->

<!--# AD - sounds good - just testing by adding my own comment here and will try and commit and push now -->

3\.
large volumes of granular spatial data <!--# FR - my argument here is that up to recently we did not have the need to deal with large data sets capturing movement. I argue that this imposes two additional challenges: (1) need to capture heterogeneity/variability across different populations and places, and (2) need for new and scalable models (where machine learning and other modelling frameworks come in) -->.

Context: spatial interaction modelling in a world of big data, machine learning, open science, digital technology and uncertainty.

The text included here is for our own benefit.
The idea of challenges

# Definition and Uses

For over 70 years, spatial interaction models (SIMs) have been the workmodel to understand spatial interactions between entities at different locations in physical space.
SIMs represent mathematical formulations through which the spatial interaction between geographic places through flows of people, information and goods can be represented.
Intuitively, these models seek to represent the spatial interaction between places as a function of three components: origin characteristics, destination characteristics and the separation between origins and destinations.
Originally adapted from physics, spatial flows between an origin and a destination were concieved to be proportional to their gravitational force and inversely related to their spatial separation.
Characteristics of origins and destinations are used to represent gravitational forces pushing and/or pulling people, information and goods from and to specific locations, and different forms of distance and cost are used to represent the deterring effects of spatial separation on spatial flows.

SIMs have been instrumental and applied in a wide range of contexts to support data analysis and decision making in retail, transport, housing, public health, land use, urban planning and population projection and forecasting contexts.
SIMs are generally used for two key purposes.
A key purpose is the *prediction* of the size and direction of spatial flows.
SIMs have been widely used to predict the impact of the development of new service units, such as shopping stores, healthcare facilities and housing units on the potential demand for associated services and traffic patterns.
Predictions from such analyses enable the identification of optimal locations and size for potential new service units.
A second key purpose of SIMs is *inference* about the factors shaping the spatial interactions in a network of flows.
SIMs have been used to determine and understand the influence of retail store on consumers' store choices and place attributes on migration decisions and commuting patterns.
SIMs have also been used to delineate geographical areas of service and retail catchment areas.

Formally SIMs take different forms.
Newtonian gravity models are probably the most widely known and used in social sciences.
Adapted from physics, the basic gravity model assumes that the interactions $T_{i j}$ between an origin $i$ and a destination $j$ in the form of flows can be understood as a function of driving forces like masses $V_{i}$ and $W_{j}$ , and a measure of spatial separation $c_{ij}$ .
Areas are assumed to interact in a positively reinforcing way that is multiplicative of their masses, and at the same time their interactions are expected to diminish with the intervening role of spatial separation.
Spatial separation is generally measured by distance, cost or time involved in the interaction, and is often represented by a distance-decay function.
The model also requires a $k$ factor to proportionality between expected and observed flows, and $\beta$ parameter representing the deterring effect of spatial separation, or distance.
The key task in a gravity model is to estimate these parameters.
A typical notation for the model is:

$$
T_{i j}=k \frac{V_{i} W_{j}}{c_{i j}^{\beta}}
$$

In practice, a matrix of flows between a set of origins, a set of destinations and a measure of spatial separation between origins and destinations is the key input for SIMs.
A family of SIMs taking four distinctive shapes is typically considered @wilson_family_1971.
A *unconstrained* formulation of the model to ensure that the total sum of the predicted flows from a gravity model be equal the total sum of the observed flows across all origins and destinations.
*Constrained* versions are used to ensure that specific restrictions are met.
Three general formulations of constrained models are used: production-constrained, attraction-constrained and doubly constrained models.
Production-constrained forms are used to constrain a model so that the predicted number of trips emanating from each origin is equal to the observed number of trips.
Attraction-constrained forms are used to constrain a model so that the predicted number of trips terminating at each destination is equal to the observed number of trips.
Doubly constrained models combine these two sets of constraints.

SIMs have been extended in four key ways.

(1) conceptualising - was not until that a theory relating to human behavior was developed for gravity model concepts and interpretations moving away from a mathemathical relationship not only predictive capability

(2) alternative developments - frameworks e.g. radiation model to account what?
discrete choice modelling to account for micro behaviour

(3) capture the effects of spatial structure on interactions - LeSage, Fotheringham, - @oshan2021spatial four generalizable approaches were proposed to account for the spatial structure effect: the CD model, the Box--Cox transform, spatial econometric models,and eigenvector spatial filtering (ESF).

-   Progress on SIMs - recognise expansions on (1) the development of the potential concept; (2) the separation of origin and destination attributes; (3) the identification of intervening opportunity and agglomeration impacts; and (4) a realization of the effects of spatial structure on interactions- and the work LeSage on spatial econometrics, Dan Griffith on spatial filtering, Kanaroglou et al on spatial discrete choice modelling, radiation models - competing destinations model Fotheringham (1983a, 1983b, 1984a).
    A large theoretical and empirical literature has developed around the definition of the "correct" exponent.

-   Remaining challenges - reproducibility (technical enabling infrastructure), calibration and leverage on the potential opportunities offered by big data - traditional and new forms of data - leverage on properties -- real-time information, large volumes and temporal frequency - machine learning to improve inference and prediction as well as address issues of scalability (suggest broad approaches: parallel computing, slicing the data), inference (discuss how p-value is not a useful indicator anymore) and capturing heterogeneity.

Objective of the section: To conceptually describe what spatial interaction modelling is (including its key components), how it is used, why it is important and how it relates to gravity modelling.
I conceive this section to provide a brief, gentle, intuitive introduction to spatial interaction models, emphasising its importance and the various context of applications of SIMs i.e. retail, population, transport, etc.

-   Mention of key SIM paradigms

<!--# FR: I would say let's introduce the intuition of traditional gravity models and then what progress has been made. -->

SIMs cover a wide range of methods and applications.
According to @rodrigue_geography_2013, there are 3 broad types of SIM:

-   Gravity models, in which interaction is estimated as a function of size/attractiveness of start/end points and some impedance function; this is the original and 'traditional' SIM.

-   Radiation models or 'potential models', in which interaction is estimated as a function of size/attractiveness of start/end points but mediated by a function of intervening opportunities [@simini_universal_2012].

-   Retail models, which seek to identify the 'market boundary' between economic hubs.

Identify reproducibility, calibration and big data as key challenges as noted above.
If we frame these as key challenges, I think it would make sense to map the rest of the sections onto these challenges.
The issue I have on doing this is that the issues of calibration and big data may entail similar or the same challenges.

\> In practice, information to operationalise SIMs is encoded in a matrix of flows between a set of origins, a set of destinations and a measure of spatial separation between origins and destinations.

# Reproducible SIMs <!--# RL to lead -->

*Objective of the section: To discuss the opportunities and challenges of estimating spatial interacting modelling in the context of reproducible research*

<!--# FR: I think this is good . The only suggestion I would make is to make this a bit broader to encapsulate the ideas of open science and enabling infrastructure. I see open science as a key enabler of reproducibility. As you identified in the text, there are options to fit SIMs but these options are commercial and not widely available, and are not fully transparent. It is challenging to know how SIMs are estimated, and therefore difficult to replicate. Also I would use the concept of enabling infrastructure because I see software as one of the components enabling the development of reproducible SIMs, but I don't think it is the only one. Again, as you wuite rightly identied below, practical tutorials to be able to apply SIMs are lacking. The work by Adam is probably an exception. Tutorials tend to focus on the theory, but the jump from the theory to the application of SIMs is quite dramatic. People often struggle to see the switch from an origin-destination matrix to a vector of origin-destination pair of flows -->

Reproducibility has not been prominent in research developing and using SIMs outlined in the previous sections.
This is understandable, because computer hardware, software and know-how needed to develop and run SIMs was simply unavailable to most people (let alone lowly and often cash-strapped students!) for most of field's history.
Even when consumer laptops became widely available and more affordable during the 2000s, there were few well-known user-friendly off-the-shelf options implementing SIMs other than MATLAB and arguably Excel, unless you were willing to dive into programming.

Fast-forward to the 2020s and computer hardware, and perhaps more importantly software, is much easier to obtain.
In terms of affordability, a second-hand laptop whose processing power would have been considered a supercomputer by 1990s standards (and impossibly powerful when seminal SIM papers were published) can be obtained for around \$100 dollars in most countries.[^1]
Tutorials teaching you to code with popular languages for data science abound, with R and Python particularly prominent in the fields (including Quantitative Geography, Social Physics, Statistics and more recently Urban Analytics) where much SIM research is undertaken.
It has never been easier to write reproducible code implementing SIMs yet, despite notable exceptions [@dennett_modelling_2018], most SIMs and the findings they produce, are not reproducible.

[^1]: See <https://ebay.com/b/Laptops-Netbooks/175672/bn_1648276> for an example of the huge international market for second-hand laptops.

One could argue that reproducibility is a 'nice to have', an optional and potentially onerous extra thing to think about during the research process.
Yet increasingly it is becoming apparent that reproducibility is *vital* for research to be falsifiable [@popper_logic_1934] and therefore scientifically sound.
In this broader context, reproducibility is a "challenge to adjust scholarly communication to today's level of digitisation and diversity of scientific outputs" [@nust_practical_2021].
A key message in our manifesto for SIM research in the 2020s, therefore, is for the models to be open and the results they create to be reproducible using published code and data (example synthetic data when the raw data cannot be published).
We urge readers implementing SIMs to make their work reproducible not only for philosophical reasons.
There are tangible benefits of making your SIM work reproducible:

-   People are more likely to cite your work if they can reproduce it.

-   Reproducible results based on open source software discourages reinvention of wheels and associated wasting of time.

-   Reproducible research encourages innovation, both of your work and the work of others, because you can focus on what is new and novel rather than (for example) writing a paper implementing an existing SIM in a slightly new context (as many SIM papers have).

To highlight the ease with which reproducible SIMs can now be developed, we present below reproducible R code that implements a simple SIM.
It is notable that open source software continues to evolve: the code presented below uses the `simodels` R package, the development of which was partly motivated by this book chapter (the primary motivation was the need to develop SIMs to represent trips for purposes other than commuting and travel to school in Ireland as part of a contract with Transport Infrastructure Ireland, highlighting the applied nature of SIM research).
`simodels` enables to develop SIMs starting with geographic datasets in fewer lines of code than was possible a few years ago [@dennett_modelling_2018].
Naturally, the starting point is to install the package (and R and a modern IDE such as RStudio or [VS Code](https://marketplace.visualstudio.com/items?itemName=REditorSupport.r) if the software is not already installed on your computer), with the following lines of code: <!--# Hopefully we can replace the second line below with # install.packages("simodels") before the chapter is published! -->

```{r, eval=FALSE}
install.packages("simodels")
```

The package installed with the previous command, which is called `simodels` (short for spatial interaction models) does not just provide functions for running and fitting (finding parameters to minimise model-observation differences): it provides a framework for developing SIMs and creating new functions implementing different types of SIM and using a variety of pre-existing modelling tools in SIMs.
We will also install `tidyverse` (if not already) for intuitive data processing functionality, and load (technically attach) the packages so their functions are available:

```{r, eval=FALSE}
install.packages("tidyverse")
```

```{r, message=FALSE}
library(simodels)
library(tidyverse)
```

The starting 'point' (pun intended!) of all SIMs is geographic entities representing trip start, end or (for 'multi-partite' models) or intermediate points.
We use the word 'features' because almost all SIMs use input datsets that are compliant with the 'simple features' open specification [@ogcopengeospatialconsortiuminc_opengis_2011], typically imported from files encoded in proprietary the Shapefile (`.shp`) or open GeoPackage (`.gpkg`), GeoJSON (`.geojson`) or other geographic file formats.
R has a mature ecosystem for working with geographic file formats, so we can use existing function for this data import stage, using the `sf` package:

```{r urls}
u_origins = "origin_zones.geojson"
f_origins = basename(u_origins)
u_destinations = "destination_points.geojson"
f_destinations = basename(u_destinations)
```

```{r, eval=FALSE}
download.file(u_origins, destfile = f_origins)
download.file(u_destinations, destfile = f_destinations)
```

```{r, echo=FALSE, eval=FALSE}
pubs_example = si_pubs %>% 
  filter(grepl(pattern = "Chemic|Nag", x = name)) %>% 
  transmute(name, size = c(100, 80))
origin_zones = si_zones %>% 
  mutate(to_pubs = round(all / runif(nrow(si_zones), min = 30, max = 70)) ) %>% 
  select(geo_code, to_pubs) 
origin_centroids = sf::st_centroid(origin_zones)
pubs_example_5km = sf::st_buffer(pubs_example, 5000)
origin_centroids_to_keep = origin_centroids[pubs_example_5km, ]
origin_zones = origin_zones %>% 
  filter(geo_code %in% origin_centroids_to_keep$geo_code)
sf::write_sf(origin_zones, "origin_zones.geojson", delete_dsn = TRUE)
sf::write_sf(pubs_example, "destination_points.geojson", delete_dsn = TRUE)
```

```{r}
origin_zones = sf::read_sf("origin_zones.geojson")
destination_points = sf::read_sf("destination_points.geojson")
```

The code chunk above demonstrates importing specific data objects <!--# FR: It may be a good idea to explain the structure of the input data as to whether the `simodels` requires a data file organised in a certain way i.e. origin-destination matrix format or can handle alternative structures.  -->:

-   A simple features object with 'multipolygon' geometries representing administrative zones that constitute trip origins in the subsequent reproducible SIMs.

-   Another simple features object with 'point' geometries representing two popular pubs in Leeds that are trip destinations in the SIMs below.

Before creating SIMs representing travel to these two pubs in Leeds, a deliberately minimal and simple input dataset to aid understanding, it is worth doing a small amount of exploratory data analysis (EDA) to check the input datasets.
This is undertaken with the following commands:

```{r, out.width="50%", fig.show='hold'}
origin_zones %>% 
  ggplot() +
  geom_histogram(aes(x = to_pubs), binwidth = 10)
origin_zones %>% 
  ggplot() +
  geom_sf(aes(fill = to_pubs), alpha = 0.5) +
  geom_sf(data = destination_points)
```

```{r, echo=FALSE, eval=FALSE}
plot(sf::st_geometry(origin_zones))
plot(sf::st_geometry(destination_points), pch = 15, add = TRUE)
```

For many applications the most important function in the `simodels` package as `si_to_od()`, as demonstrated below:

```{r}
od_zones_to_points = si_to_od(origin_zones, destination_points)
class(od_zones_to_points)
nrow(od_zones_to_points)
names(od_zones_to_points)
```

As shown in the output above, the result is a data frame with 94 rows (representing the full combination of trips from every one of the 47 origin zone to each of the 2 destinations).
The names of the data frame refer to variables for each origin and each destination.
When working on large input datasets, this 'full matrix' of combinations can get unhelpfully large: an OD dataset from every MSOA to every pub in England, for example, would results in a dataset with `r format(7000 * 50000, big.mark = ",", scientific = FALSE)` (350 million) rows, for example.
To reduce dataset sizes, by orders of magnitude in some cases, the 'sparse matrix' representing only OD pairs below a certain distance threshold can be created instead of the full matrix, by adding a `max_dist` argument as follows, which sets the maximum Euclidean distance between zone centroids and point destinations to be included in the OD dataset at 5 km in this case:

```{r}
od_zones_to_points = si_to_od(origin_zones, destination_points, max_dist = 5000)
nrow(od_zones_to_points)
```

This updated OD dataset is only a bit smaller (79 rows compared with 94 rows previously) of the size of the original because the example dataset is samll but in other cases setting a maximum distance can greatly speed-up SIM processing, modelling and visualisation run times in cases where you are happy to treat trips more than a threshold distance as negligible (in the case of trips to pubs 5 km is probably reasonable for distances to 'the local' pub in urban areas but the distance threshold should be based on evidence where possible).

From this point, we can specific a SIM model as a function as follows:

```{r}
gravity_model = function(beta, d, m, n) {
  m * n * exp(-beta * d / 1000)
} 
```

and implement it with the following command:

```{r}
od_to_pubs_result = od_zones_to_points %>% 
  si_calculate(fun = gravity_model, 
               m = origin_to_pubs,
               n = destination_size,
               d = distance_euclidean,
               beta = 0.5,
               constraint_production = origin_to_pubs)
```

We can check the results as follows:

```{r}
sum(od_to_pubs_result$interaction)
sum(origin_zones$to_pubs)
```

As shown above, the total number of trips is the same in the OD data as in the zone level data.
We can visualise the result as follows, resulting in Figure \@ref(fig:pubresmap):

```{r pubresmap, message=FALSE, fig.cap="Results of a reproducible SIM undertaken on a minimal example based on synthetic data representing hypothetical trips to 2 pubs in Leeds, UK."}
library(ggspatial)
# rosm::osm.types()
od_to_pubs_result %>% 
  ggplot() +
  annotation_map_tile(type = "cartolight") +
  geom_sf(aes(lwd = interaction, colour = D), alpha = 0.5) +
  scale_size_continuous(range = c(0.3, 3)) +
  geom_sf(data = origin_zones, fill = NA, lty = 2, alpha = 0.5) +
  theme_void()
```

```{r, echo=FALSE, eval=FALSE}
# commented out as not helpful result: we would need more destinations for this to be useful:
od_to_pubs_result %>% 
  ggplot() +
  geom_point(aes(distance_euclidean, interaction, colour = D))
```

# Calibration

As Openshaw (1975) succinctly puts it, "calibration is the process of providing estimates of the unknown parameters we have identified as the independent variables of the model".
In a basic gravity formulation of a spatial interaction model there will be three parameters to estimate relating to origin mass or emissivity, destination mass or attractiveness and a parameter determining the frictional effect of the cost of interactions occurring between the two.

The process of calibration allows the basic formulation of the model to be tweaked by adjusting the values of these parameters according to whatever context the model is being used in so that the estimates produced by the model correspond as closely as possible to any observations or data that a researcher might have.
In doing so, as well as producing better estimates from our model, we learn something about our system through the parameter values themselves.
For example, the one parameter that appears in all spatial interaction models relates to the cost exponent.
Almost universally described in the literature with the symbol $\beta$ and almost always negative, the value of the beta parameter in a given system reveals the frictional effect that the cost of interaction has on the flows being observed, with higher negative values indicating a more severe deterrent.
Parameters associated with other variables, depending on the model, tell the story of the elements of the system and allow us to make judgements about what might be leading to the behaviours we observe -- why socio economic status may influence shopping expenditure, for example.

The big challenge is that calibration requires both data and -- particularly where there are multiple parameters to calibrate - software.
Forty or fifty years ago when many of the theoretical foundations of spatial interaction modelling were laid, the data landscape was somewhat different from today -- flow data were rarely available in volume and certainly not at the sort of temporal rhythm they are now where, for example, supermarket loyalty-card holders generate origin/destination revenue flow data from residential origins to store destinations at daily frequencies over time periods than can cover many years or even decades.
As such, we might be forgiven for expecting that even if the science and theory underpinning the models has not developed very much, the software and processes facilitating calibration -- relating empirical observations to the theoretical representations embodied in the models -- might have.
But in many ways, they have not.

Today it's not uncommon for flow data to be collected for various purposes.
As already mentioned it exists where any retailer has a loyalty card expenditure linked to a customer's home address and the store they shop in.
It exists in the public sector where, for example, school roll information is held with the home addresses of pupils.
It exists in the databases of companies like SafeGraph who derive trip data between homes and various point of interest destinations from mobile phone locational traces.
Accessing interaction data for a system of interest is not the barrier it once was, even if the proprietary nature of some of it can still present some access challenges.
However, for anyone who is in possession of these data, making sense of it -- understanding where commuting flows are unexpected, or why some retail stores of apparently similar composition have wildly different income profiles -- is more of a challenge.

Much of the challenge is because there is a dearth of knowledge through of a lack of exposure within geographical education.
Despite these models underpinning many of the processes that interest geographers, spatial interaction models are not taught to undergraduate geography students in the same way that, say, regression models are taught to economists or social psychologists.
As such, it is not immediately obvious, even for those coming out of geography degree programmes which might incorporate some level of 'GIS' education, how anyone might go about fitting their spatial data to a theoretical model, calibrating the parameters and revealing properties of their system.

The question the follows, then, is why have cohorts of undergraduate geography students not been taught how to fit these models and explore systems of spatial interaction when it has been standard practice to show them how to produce choropleth maps on a computer or learn to apply a post-colonial lens to the interpretation of a text?
The answer has to come down to a problem of accessibility.
We discuss elsewhere in this chapter the fact that reproducibility has been an issue in the field, but that with packages like SPINT (\*Oshan reference) and our own efforts with the simodels package in R, the tide is beginning to turn.
However, calibration remains a challenging sub-topic for reproducibility and the wider accessibility of these models, particularly where algorithms and routines which are able to calibrate parameters have remained locked away - either behind dense algebraic notation in dusty papers from the 1970s, or where they have found their way into software, behind paywalls as in the case of the Huff Model calibration routines found in ArcGIS but obscured to most behind the additional 'Business Analyst' licence.

Ironically, perfectly effective calibration routines have been available for as long as students have been running regression models in their introductory statistics classes.
Occasional references can be found in the historic literature such as in papers by Fotheringham and Webber (1980) and Flowerdew and Aitken (1982) which lift the curtain and reveal that through reformulating the classic Wilsonian entropy maximising spatial interaction model as either a logged OLS regression model or a generalised linear model utilising a Poisson or negative binomial distributions, multiple parameters can be calibrated easily.
These models are all available in common statistical software packages and implement algorithms such as iteratively reweighted least squares to find the maximum likelihood parameter values, but for most trying to make sense of the field coming across papers by some of the doyens (for it was and still is a male-dominated field) of the scene such as Wilson, Fotheringham, Pooler, Openshaw and Roy, while undoubtedly mathematically and theoretically rigorous, notes on calibration were at best reduced to a passing reference to 'least squares' or at worst a lengthy derivation of maximum likelihood of Newton Raphson methods.

Compounding the calibration challenge further, myriad proprietary data formats emerged during the 1980s and 90s with rarely the same file-type used for 'GIS' analysis as statistical modelling.
Â A melange that has meant that for many the overheads involved just in converting data between SPSS, Minitab, Stata, ArcGIS, MapInfo will have been too severe to contemplate.
Even for those fortunate enough to have received a grounding in both spatial analysis methods and more general statistical techniques this will have not helped any mental balkanisation perhaps already occurring through a lack of practical education where regression may have been taught entirely separately from spatial modelling.

Calibration is at the centre of what makes spatial interaction models useful to researchers and we have outlined why challenges emerged and persisted in the field.
But we are confident that with software such as R and Python meaning that statistical and geographic analyses are no longer separated in the workflow and packages emerging, contributed to by researchers active in the field enabling others to run reproducible analyses, these challenges need not persist.
With increasing volumes of data coming online to allow multiple different sorts of models to be tested and calibrated against empirical observations in a variety of different contexts, we will be able to find solutions to problems that inevitably emerge when calibrating general models to messy reality: issues of sparse or huge interaction matrices.
Issues of multiple scales, dimensions and aggregations.
Issues of spatial non-stationarity.
Issues of incomplete, poor or otherwise imperfect calibration data.
These are all problems that persist but that due to new researchers in the field invariably starting from scratch rather than usefully building on existing knowledge through accessible methods, have not yet been successfully solved.

# Expanding spatial interaction modelling

Objective of the section: To discuss ways in which spatial interaction modelling can be expanded.
I am particularly interested in advocating for (1) seeing spatial interaction models in a context of uncertainty, rather than as mathematical models; and, (2) discussing how hierarchical / generalised linear mixed modelling can offer ways to capture variability across places and populations.

# Spatial interaction modelling using machine learning

Objective of the section: To discuss how machine learning can enhance flow count inference and prediction

# Facilitating future progress of spatial interaction modelling

Objective of the section: To identify and discuss the key pillars that will enable progress on all the proposed fronts - open science, important and new questions and digital infrastructure.
I see this as our conclusion - probably one or two short paragraph summarising what has been discussed with a forward looking approach.

# References
